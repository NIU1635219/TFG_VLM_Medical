# TFG: GeneraciÃ³ de Descripcions Explicatives en Imatges MÃ¨diques amb VLMs

**Estudiant:** David Bonilla Medina  
**Grau:** Enginyeria InformÃ tica (MenciÃ³ ComputaciÃ³) - UAB  
**Curs:** 2025/2026

## ğŸ“‹ DescripciÃ³
Aquest projecte explora l'Ãºs de **Models de Llenguatge Visual (VLMs)** d'Ãºltima generaciÃ³ (SOTA 2026) i arquitectura *Open Source* per generar descripcions clÃ­niques detallades (*Explainability*) d'imatges de colonoscÃ²pia (pÃ²lips).

S'analitzaran i compararan models com **Qwen3-VL**, **MiniCPM-V 4.5** i **InternVL 3.5**, centrant-se en l'Ãºs de noves tecnologies d'encoder visual i mecanismes de resoluciÃ³ dinÃ mica.

L'objectiu Ã©s demostrar la viabilitat d'executar aquests sistemes en **entorns locals** (Edge AI) utilitzant maquinari de consum, garantint la privacitat de les dades mÃ¨diques mitjanÃ§ant el backend d'inferÃ¨ncia **Ollama**.

## ğŸ› ï¸ Stack TecnolÃ²gic
*   **Llenguatge:** Python 3.12
*   **Gestor de Paquets:** `uv` (Astral)
*   **Backend d'Inferencia:** [Ollama](https://ollama.com/) (Local API)
*   **Llibreries Clau:** `ollama-python`, `pillow`, `requests`, `tqdm`.
*   **Entorn:** Lightweight (sense dependÃ¨ncies pesades de PyTorch/Transformers en el llanÃ§ador).

## ğŸ› ï¸ Manager Tool (v5.0) - CLI Interactive
El projecte inclou una potent eina de gestiÃ³ (`setup_env.py`) amb una **InterfÃ­cie d'Usuari de Text (TUI)** avanÃ§ada que facilita la configuraciÃ³, el diagnÃ²stic i el manteniment de l'entorn Ollama.

**Funcionalitats Principals:**
*   **ğŸ® InterfÃ­cie GrÃ fica en Terminal:** NavegaciÃ³ intuÃ¯tiva, circular i sense parpadeigs (*Flicker-Free*).
*   **ğŸ§­ DescripciÃ³ contextual d'opcions:** Mostra una descripciÃ³ curta de l'opciÃ³ seleccionada en tots els menÃºs.
*   **ğŸ©º DiagnÃ²stic DinÃ mic:** Analitza l'estat de les llibreries crÃ­tiques i la connexiÃ³ amb el servei Ollama.
*   **ğŸ¤– GestiÃ³ de Models VLM:** Sistema integrat per descarregar (*pull*) models directament des del registre d'Ollama.
*   **ğŸ§ª Smoke Test (Auto + Interactiu):** Prova d'inferÃ¨ncia amb mÃºltiples imatges neutres, descÃ rrega automÃ tica i validaciÃ³ per paraules clau.
*   **ğŸ›¡ï¸ Factory Reset segur:** La confirmaciÃ³ de `Factory Reset` ve per defecte en **No** per evitar reinicis accidentals d'entorn.
*   **ğŸ”„ Auto-Bootstrapping:** El sistema detecta automÃ ticament si s'estÃ  executant fora de l'entorn virtual (`.venv`) i es reinicia dins d'ell per garantir la cÃ rrega de llibreries.

**Controls:**
*   `â¬†ï¸` / `â¬‡ï¸`: Navegar per les opcions.
*   `ESPAI`: Entrar en SubmenÃº / Marcar o desmarcar opcions.
*   `ESC`: Tornar enrere o sortir.
*   `ENTER`: **Confirmar i Executar** la selecciÃ³ actual.

## ğŸš€ InstalÂ·laciÃ³ i ConfiguraciÃ³

1.  **Prerequisit: InstalÂ·lar Ollama**
    Descarrega i instalÂ·la Ollama des de [ollama.com](https://ollama.com). Assegura't que el servei estigui actiu (`ollama serve`).

2.  **Clonar el repositori:**
    ```bash
    git clone https://github.com/NIU1635219/TFG_VLM_Medical.git
    cd TFG_VLM_Medical
    ```

3.  **Executar el Manager Tool:**
    ```bash
    # Windows
    .\setup.bat
    ```
    L'script configurarÃ  l'entorn virtual, instalÂ·larÃ  les dependÃ¨ncies i obrirÃ  el menÃº de gestiÃ³.

## ğŸ“‚ Estructura del Projecte
L'arquitectura del projecte estÃ  dissenyada per ser modular:
*   `src/inference/`: Controladors d'inferÃ¨ncia basats en l'API d'Ollama.
*   `src/scripts/`: Utilitats de terminal (test d'inferÃ¨ncia interactiva, etc).
*   `tests/`: Tests unitaris i d'integraciÃ³ (Pytest).
*   `data/`: dataset mÃ¨dic segmentat en `raw/` i `processed/`.
*   `setup_env.py`: Script de gestiÃ³ v5.0 (TUI).

## ğŸ¤– Models VLM
L'execuciÃ³ d'inferÃ¨ncia i els tests **detecten dinÃ micament** els models disponibles via `ollama list`.

La llista segÃ¼ent es mantÃ© com a **registre de models recomanats per descarregar** des del manager (`setup_env.py`), no com a llista fixa d'execuciÃ³:
| Model | Tag en Ollama | DescripciÃ³ |
| :--- | :--- | :--- |
| **MiniCPM-V 4.5** | `openbmb/minicpm-v4.5:8b` | SOTA OpenBMB (8B) |
| **MiniCPM-V 2.6** | `openbmb/minicpm-v2.6:8b` | VersiÃ³ Estable (8B) |
| **Qwen3-VL** | `qwen3-vl:8b` | SOTA Razonamiento 2026 (8B) |
| **InternVL 3.5** | `blaifa/InternVL3_5:8b` | InternVL High Performance (8B) |

## ğŸ§ª Testing
Per executar els tests unitaris i verificar la integraciÃ³ amb Ollama:
```bash
uv run python -m pytest tests/
```

Smoke test automÃ tic (no interactiu):
```bash
uv run python src/scripts/test_inference.py
```

Smoke test interactiu (selector de model):
```bash
uv run python src/scripts/test_inference.py --interactive
```

Notes del smoke test:
* Usa 4 imatges amb noms neutres (`sample_01.jpg` ... `sample_04.jpg`) a `data/raw/smoke_test/`.
* Si no existeixen, les descarrega automÃ ticament des de mÃºltiples URLs fallback i les normalitza.
* PrecÃ rrega el model una sola vegada abans del bucle de casos i l'allibera en acabar (reduint latÃ¨ncia per cas).
* Valida automÃ ticament que la resposta del model inclogui paraules clau esperades per cada imatge.

Notes del selector al Manager Tool:
* A `Tests & Models Manager > Run Smoke Test`, nomÃ©s es mostren models detectats via `ollama list` (sense entrada manual de tag en aquest menÃº).
